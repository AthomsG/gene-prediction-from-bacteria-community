{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab2b573-1087-40e7-bcef-1410f0be3c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a338b0-d381-444b-bafc-39a774df1899",
   "metadata": {},
   "source": [
    "# Sparsity Aware Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ad9f7c-03ed-45d8-88aa-38a804a595f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualBranchNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, num_hidden_layers=2, dropout_rate=0.3):\n",
    "        super(DualBranchNet, self).__init__()\n",
    "\n",
    "        # Dynamically create shared feature extractor layers\n",
    "        shared_layers = []\n",
    "        in_features = input_dim\n",
    "        for _ in range(num_hidden_layers):\n",
    "            shared_layers.append(nn.Linear(in_features, hidden_dim))\n",
    "            shared_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            shared_layers.append(nn.LeakyReLU())\n",
    "            shared_layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = hidden_dim  # Update input size for next layer\n",
    "        \n",
    "        self.shared_layers = nn.Sequential(*shared_layers)\n",
    "\n",
    "        # Classification branch (Binary classification for P(Y > 0))\n",
    "        self.classification_branch = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),  # Output a single probability per target\n",
    "            nn.Sigmoid()  # Outputs probability P(Y > 0)\n",
    "        )\n",
    "\n",
    "        # Regression branch (Predicts E[Y | Y > 0])\n",
    "        self.regression_branch = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.ReLU()  # Ensures non-negative output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.log1p(x) # log transform X\n",
    "        features = self.shared_layers(x)\n",
    "        prob_y_positive = self.classification_branch(features)  # Probability P(Y > 0)\n",
    "        y_regression = self.regression_branch(features)  # Expected Y given Y > 0\n",
    "        return prob_y_positive, y_regression\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Final Prediction: P(Y > 0) * E[Y | Y > 0]\"\"\"\n",
    "        prob_y_positive, y_regression = self.forward(x)\n",
    "        return prob_y_positive * y_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc3c12-2bc5-4515-959c-f1e3f6a37884",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78819b1e-c11f-4bde-9435-e32d6dd47735",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6f5370-4b09-4b72-9115-88e2d5ffa625",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxon = pd.read_parquet('../processed_data/taxon_coverage_s.parquet')\n",
    "\n",
    "if full_data:\n",
    "    taxons = ['s', 'g', 'f', 'c', 'o', 'p']\n",
    "    for taxon in taxons:\n",
    "        df_temp = pd.read_parquet(f'../processed_data/taxon_coverage_{taxon}.parquet')\n",
    "        df_taxon = pd.concat([df_taxon, df_temp], axis=1)  # Concatenate along columns\n",
    "\n",
    "df_genes = pd.read_parquet('../processed_data/gene_coverage_95.parquet')\n",
    "df_genes = df_genes.set_index(\"Sample\")\n",
    "\n",
    "# force same order \n",
    "df_genes = df_genes.reindex(df_taxon.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44be5424-262e-47bb-a0ed-ca770a48f3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bifidobacterium longum</th>\n",
       "      <th>Bifidobacterium infantis</th>\n",
       "      <th>Bifidobacterium breve</th>\n",
       "      <th>Bifidobacterium bifidum</th>\n",
       "      <th>Collinsella sp022728415</th>\n",
       "      <th>Bacteroides fragilis</th>\n",
       "      <th>Bacteroides stercoris</th>\n",
       "      <th>Bacteroides caccae</th>\n",
       "      <th>Phocaeicola vulgatus</th>\n",
       "      <th>Prevotella sp900543975</th>\n",
       "      <th>...</th>\n",
       "      <th>Caldisericota</th>\n",
       "      <th>UBA10199</th>\n",
       "      <th>Desulfobacterota_B</th>\n",
       "      <th>Myxococcota_A</th>\n",
       "      <th>KSB1</th>\n",
       "      <th>Cloacimonadota</th>\n",
       "      <th>Hydrogenedentota</th>\n",
       "      <th>Desulfobacterota_G</th>\n",
       "      <th>Riflebacteria</th>\n",
       "      <th>Desulfobacterota_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ERR2835298</th>\n",
       "      <td>85.91</td>\n",
       "      <td>152.43</td>\n",
       "      <td>109.57</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.61</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.72</td>\n",
       "      <td>23.53</td>\n",
       "      <td>11.86</td>\n",
       "      <td>2.87</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR12302558</th>\n",
       "      <td>63.99</td>\n",
       "      <td>31.05</td>\n",
       "      <td>113.22</td>\n",
       "      <td>18.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>66.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR13622751</th>\n",
       "      <td>125.85</td>\n",
       "      <td>99.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR15127829</th>\n",
       "      <td>419.87</td>\n",
       "      <td>545.92</td>\n",
       "      <td>55.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR12325043</th>\n",
       "      <td>109.82</td>\n",
       "      <td>287.97</td>\n",
       "      <td>5.50</td>\n",
       "      <td>31.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERR3053399</th>\n",
       "      <td>0.00</td>\n",
       "      <td>165.16</td>\n",
       "      <td>9.36</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR13774645</th>\n",
       "      <td>0.00</td>\n",
       "      <td>544.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR13774552</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1196.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR13774476</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1986.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR13774639</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2391.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2690 rows × 11254 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Bifidobacterium longum  Bifidobacterium infantis  \\\n",
       "ERR2835298                    85.91                    152.43   \n",
       "SRR12302558                   63.99                     31.05   \n",
       "SRR13622751                  125.85                     99.75   \n",
       "SRR15127829                  419.87                    545.92   \n",
       "SRR12325043                  109.82                    287.97   \n",
       "...                             ...                       ...   \n",
       "ERR3053399                     0.00                    165.16   \n",
       "SRR13774645                    0.00                    544.66   \n",
       "SRR13774552                    0.00                   1196.35   \n",
       "SRR13774476                    0.00                   1986.75   \n",
       "SRR13774639                    0.00                   2391.46   \n",
       "\n",
       "             Bifidobacterium breve  Bifidobacterium bifidum  \\\n",
       "ERR2835298                  109.57                     8.24   \n",
       "SRR12302558                 113.22                    18.55   \n",
       "SRR13622751                   0.00                     0.00   \n",
       "SRR15127829                  55.28                     0.00   \n",
       "SRR12325043                   5.50                    31.63   \n",
       "...                            ...                      ...   \n",
       "ERR3053399                    9.36                     9.84   \n",
       "SRR13774645                   0.00                     0.00   \n",
       "SRR13774552                   0.00                     0.00   \n",
       "SRR13774476                   0.00                     0.00   \n",
       "SRR13774639                   0.00                     0.00   \n",
       "\n",
       "             Collinsella sp022728415  Bacteroides fragilis  \\\n",
       "ERR2835298                      0.61                  5.30   \n",
       "SRR12302558                     0.00                 66.84   \n",
       "SRR13622751                     0.00                  0.00   \n",
       "SRR15127829                     0.00                  0.00   \n",
       "SRR12325043                     0.00                  0.00   \n",
       "...                              ...                   ...   \n",
       "ERR3053399                      0.00                  0.00   \n",
       "SRR13774645                     0.00                  0.00   \n",
       "SRR13774552                     0.00                  0.00   \n",
       "SRR13774476                     0.00                  0.00   \n",
       "SRR13774639                     0.00                  0.00   \n",
       "\n",
       "             Bacteroides stercoris  Bacteroides caccae  Phocaeicola vulgatus  \\\n",
       "ERR2835298                    1.72               23.53                 11.86   \n",
       "SRR12302558                   0.00                0.00                  0.00   \n",
       "SRR13622751                   0.00                0.00                  0.00   \n",
       "SRR15127829                   0.00                0.00                  0.00   \n",
       "SRR12325043                   0.00                0.00                  0.00   \n",
       "...                            ...                 ...                   ...   \n",
       "ERR3053399                    0.00                0.00                  0.00   \n",
       "SRR13774645                   0.00                0.00                  0.00   \n",
       "SRR13774552                   0.00                0.00                  0.00   \n",
       "SRR13774476                   0.00                0.00                  0.00   \n",
       "SRR13774639                   0.00                0.00                  0.00   \n",
       "\n",
       "             Prevotella sp900543975  ...  Caldisericota  UBA10199  \\\n",
       "ERR2835298                     2.87  ...            0.0       0.0   \n",
       "SRR12302558                    0.00  ...            0.0       0.0   \n",
       "SRR13622751                    0.00  ...            0.0       0.0   \n",
       "SRR15127829                    0.00  ...            0.0       0.0   \n",
       "SRR12325043                    0.00  ...            0.0       0.0   \n",
       "...                             ...  ...            ...       ...   \n",
       "ERR3053399                     0.00  ...            0.0       0.0   \n",
       "SRR13774645                    0.00  ...            0.0       0.0   \n",
       "SRR13774552                    0.00  ...            0.0       0.0   \n",
       "SRR13774476                    0.00  ...            0.0       0.0   \n",
       "SRR13774639                    0.00  ...            0.0       0.0   \n",
       "\n",
       "             Desulfobacterota_B  Myxococcota_A  KSB1  Cloacimonadota  \\\n",
       "ERR2835298                  0.0            0.0   0.0             0.0   \n",
       "SRR12302558                 0.0            0.0   0.0             0.0   \n",
       "SRR13622751                 0.0            0.0   0.0             0.0   \n",
       "SRR15127829                 0.0            0.0   0.0             0.0   \n",
       "SRR12325043                 0.0            0.0   0.0             0.0   \n",
       "...                         ...            ...   ...             ...   \n",
       "ERR3053399                  0.0            0.0   0.0             0.0   \n",
       "SRR13774645                 0.0            0.0   0.0             0.0   \n",
       "SRR13774552                 0.0            0.0   0.0             0.0   \n",
       "SRR13774476                 0.0            0.0   0.0             0.0   \n",
       "SRR13774639                 0.0            0.0   0.0             0.0   \n",
       "\n",
       "             Hydrogenedentota  Desulfobacterota_G  Riflebacteria  \\\n",
       "ERR2835298                0.0                 0.0            0.0   \n",
       "SRR12302558               0.0                 0.0            0.0   \n",
       "SRR13622751               0.0                 0.0            0.0   \n",
       "SRR15127829               0.0                 0.0            0.0   \n",
       "SRR12325043               0.0                 0.0            0.0   \n",
       "...                       ...                 ...            ...   \n",
       "ERR3053399                0.0                 0.0            0.0   \n",
       "SRR13774645               0.0                 0.0            0.0   \n",
       "SRR13774552               0.0                 0.0            0.0   \n",
       "SRR13774476               0.0                 0.0            0.0   \n",
       "SRR13774639               0.0                 0.0            0.0   \n",
       "\n",
       "             Desulfobacterota_D  \n",
       "ERR2835298                  0.0  \n",
       "SRR12302558                 0.0  \n",
       "SRR13622751                 0.0  \n",
       "SRR15127829                 0.0  \n",
       "SRR12325043                 0.0  \n",
       "...                         ...  \n",
       "ERR3053399                  0.0  \n",
       "SRR13774645                 0.0  \n",
       "SRR13774552                 0.0  \n",
       "SRR13774476                 0.0  \n",
       "SRR13774639                 0.0  \n",
       "\n",
       "[2690 rows x 11254 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0300f1-5ba4-4e7a-8977-338a94056b73",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d863e1a0-608f-40b5-85fa-78327e4df628",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_genes   = (df_genes > 0).sum(0)\n",
    "count_species = (df_taxon > 0).sum(0)\n",
    "\n",
    "threshold = 200 # remove genes and taxon that show up less that threshold times in the data\n",
    "\n",
    "df_taxon = df_taxon.loc[:, count_species > threshold]\n",
    "df_genes = df_genes.loc[:, count_genes > threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd45bf-924b-4eb0-93fe-1c0e538225d8",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71647f80-1540-4399-abf9-9d747ec9bf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 118,185,332 parameters\n"
     ]
    }
   ],
   "source": [
    "model = DualBranchNet(input_dim=df_taxon.shape[1], \n",
    "                      hidden_dim=1024, \n",
    "                      num_hidden_layers=4,\n",
    "                      dropout_rate=0.3,\n",
    "                      output_dim=df_genes.shape[1])\n",
    "\n",
    "num_params = sum(param.numel() for param in model.parameters())\n",
    "\n",
    "print(f'Model has {num_params:,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef384f-abc6-4285-9d83-292bfcbd391d",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033940e1-74df-4197-9cde-88bec3a69278",
   "metadata": {},
   "source": [
    "## Data to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea17269d-b214-4076-9316-8703199c6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(df_taxon.values, dtype=torch.float32)\n",
    "Y = torch.tensor(df_genes.values, dtype=torch.float32)\n",
    "\n",
    "# train val test split\n",
    "train_ratio = 0.7\n",
    "val_ratio   = 0.15\n",
    "test_ration = 1 - (train_ratio + val_ratio) # doesn't matter because we'll keep the rest\n",
    "\n",
    "num_samples = X.shape[0]\n",
    "train_size = int(train_ratio * num_samples)\n",
    "val_size = int(val_ratio * num_samples)\n",
    "test_size = num_samples - train_size - val_size\n",
    "\n",
    "indices = torch.randperm(num_samples)\n",
    "train_idx, val_idx, test_idx = indices[:train_size], indices[train_size:train_size+val_size], indices[train_size+val_size:]\n",
    "\n",
    "X_train, Y_train = X[train_idx], Y[train_idx]\n",
    "X_val, Y_val = X[val_idx], Y[val_idx]\n",
    "X_test, Y_test = X[test_idx], Y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a9252-772e-47b7-afa6-b46dc779893d",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54e60e-34cd-4ec9-be1c-76ef8d183669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "  Train -> BCE: 0.6663, MSE: 21902.4569, Jaccard: 0.4290\n",
      "  Val   -> BCE: 0.6082, MSE: 22905.1850, Jaccard: 0.5437\n",
      "Epoch 2/600\n",
      "  Train -> BCE: 0.5975, MSE: 18191.4605, Jaccard: 0.5731\n",
      "  Val   -> BCE: 0.5552, MSE: 14060.4907, Jaccard: 0.6005\n",
      "Epoch 3/600\n",
      "  Train -> BCE: 0.5399, MSE: 17522.7514, Jaccard: 0.5937\n",
      "  Val   -> BCE: 0.5160, MSE: 12730.5093, Jaccard: 0.5973\n",
      "Epoch 4/600\n",
      "  Train -> BCE: 0.5175, MSE: 17086.7506, Jaccard: 0.5837\n",
      "  Val   -> BCE: 0.5122, MSE: 12442.4983, Jaccard: 0.5987\n",
      "Epoch 5/600\n",
      "  Train -> BCE: 0.5131, MSE: 16661.4962, Jaccard: 0.5835\n",
      "  Val   -> BCE: 0.5080, MSE: 12455.1343, Jaccard: 0.5813\n",
      "Epoch 6/600\n",
      "  Train -> BCE: 0.5054, MSE: 17612.5198, Jaccard: 0.5928\n",
      "  Val   -> BCE: 0.4895, MSE: 12432.3045, Jaccard: 0.6032\n",
      "Epoch 7/600\n",
      "  Train -> BCE: 0.4944, MSE: 16724.2488, Jaccard: 0.5995\n",
      "  Val   -> BCE: 0.4776, MSE: 12839.8173, Jaccard: 0.6161\n",
      "Epoch 8/600\n",
      "  Train -> BCE: 0.4884, MSE: 17078.9380, Jaccard: 0.6023\n",
      "  Val   -> BCE: 0.4765, MSE: 12517.3417, Jaccard: 0.6053\n",
      "Epoch 9/600\n",
      "  Train -> BCE: 0.4817, MSE: 16502.1423, Jaccard: 0.6067\n",
      "  Val   -> BCE: 0.4754, MSE: 12253.5698, Jaccard: 0.6157\n",
      "Epoch 10/600\n",
      "  Train -> BCE: 0.4851, MSE: 16475.5413, Jaccard: 0.6054\n",
      "  Val   -> BCE: 0.4782, MSE: 12710.8098, Jaccard: 0.6055\n",
      "Epoch 11/600\n",
      "  Train -> BCE: 0.4835, MSE: 16518.4211, Jaccard: 0.6057\n",
      "  Val   -> BCE: 0.4662, MSE: 12614.4178, Jaccard: 0.6143\n",
      "Epoch 12/600\n",
      "  Train -> BCE: 0.4799, MSE: 16874.9764, Jaccard: 0.6090\n",
      "  Val   -> BCE: 0.4747, MSE: 12846.3412, Jaccard: 0.6112\n",
      "Epoch 13/600\n",
      "  Train -> BCE: 0.4790, MSE: 16058.3765, Jaccard: 0.6098\n",
      "  Val   -> BCE: 0.4736, MSE: 12669.3837, Jaccard: 0.6214\n",
      "Epoch 14/600\n",
      "  Train -> BCE: 0.4802, MSE: 16208.0903, Jaccard: 0.6089\n",
      "  Val   -> BCE: 0.4731, MSE: 12667.5747, Jaccard: 0.6171\n",
      "Epoch 15/600\n",
      "  Train -> BCE: 0.4803, MSE: 15907.7457, Jaccard: 0.6079\n",
      "  Val   -> BCE: 0.4728, MSE: 12930.3996, Jaccard: 0.6159\n",
      "Epoch 16/600\n",
      "  Train -> BCE: 0.4826, MSE: 16141.0386, Jaccard: 0.6100\n",
      "  Val   -> BCE: 0.4723, MSE: 12763.6378, Jaccard: 0.6138\n",
      "Epoch 17/600\n",
      "  Train -> BCE: 0.4799, MSE: 16602.2800, Jaccard: 0.6127\n",
      "  Val   -> BCE: 0.4828, MSE: 12936.9287, Jaccard: 0.5943\n",
      "Epoch 18/600\n",
      "  Train -> BCE: 0.4789, MSE: 16421.3299, Jaccard: 0.6132\n",
      "  Val   -> BCE: 0.4777, MSE: 12665.3766, Jaccard: 0.6202\n",
      "Epoch 19/600\n",
      "  Train -> BCE: 0.4781, MSE: 15941.3063, Jaccard: 0.6135\n",
      "  Val   -> BCE: 0.4648, MSE: 12614.2205, Jaccard: 0.6256\n",
      "Epoch 20/600\n",
      "  Train -> BCE: 0.4738, MSE: 15911.2093, Jaccard: 0.6170\n",
      "  Val   -> BCE: 0.4752, MSE: 12472.8360, Jaccard: 0.6172\n",
      "Epoch 21/600\n",
      "  Train -> BCE: 0.4753, MSE: 15848.5399, Jaccard: 0.6158\n",
      "  Val   -> BCE: 0.4666, MSE: 12613.9304, Jaccard: 0.6238\n",
      "Epoch 22/600\n",
      "  Train -> BCE: 0.4717, MSE: 15530.6054, Jaccard: 0.6192\n",
      "  Val   -> BCE: 0.4680, MSE: 12426.4330, Jaccard: 0.6145\n",
      "Epoch 23/600\n",
      "  Train -> BCE: 0.4688, MSE: 16088.2706, Jaccard: 0.6197\n",
      "  Val   -> BCE: 0.4596, MSE: 12376.1808, Jaccard: 0.6289\n",
      "Epoch 24/600\n",
      "  Train -> BCE: 0.4653, MSE: 14929.0385, Jaccard: 0.6226\n",
      "  Val   -> BCE: 0.4593, MSE: 12313.9910, Jaccard: 0.6321\n",
      "Epoch 25/600\n",
      "  Train -> BCE: 0.4649, MSE: 15330.6629, Jaccard: 0.6252\n",
      "  Val   -> BCE: 0.4605, MSE: 12025.4354, Jaccard: 0.6286\n",
      "Epoch 26/600\n",
      "  Train -> BCE: 0.4667, MSE: 15322.5905, Jaccard: 0.6226\n",
      "  Val   -> BCE: 0.4582, MSE: 12006.0873, Jaccard: 0.6365\n",
      "Epoch 27/600\n",
      "  Train -> BCE: 0.4617, MSE: 14749.6499, Jaccard: 0.6242\n",
      "  Val   -> BCE: 0.4497, MSE: 11951.0995, Jaccard: 0.6205\n",
      "Epoch 28/600\n",
      "  Train -> BCE: 0.4602, MSE: 14679.4381, Jaccard: 0.6244\n",
      "  Val   -> BCE: 0.4570, MSE: 12247.8808, Jaccard: 0.6223\n",
      "Epoch 29/600\n",
      "  Train -> BCE: 0.4572, MSE: 14570.3523, Jaccard: 0.6289\n",
      "  Val   -> BCE: 0.4568, MSE: 11928.6359, Jaccard: 0.6329\n",
      "Epoch 30/600\n",
      "  Train -> BCE: 0.4596, MSE: 13832.8368, Jaccard: 0.6264\n",
      "  Val   -> BCE: 0.4574, MSE: 11763.6150, Jaccard: 0.6337\n",
      "Epoch 31/600\n",
      "  Train -> BCE: 0.4602, MSE: 13782.6512, Jaccard: 0.6287\n",
      "  Val   -> BCE: 0.4522, MSE: 11654.3684, Jaccard: 0.6325\n",
      "Epoch 32/600\n",
      "  Train -> BCE: 0.4598, MSE: 13981.2264, Jaccard: 0.6280\n",
      "  Val   -> BCE: 0.4602, MSE: 11614.0091, Jaccard: 0.6357\n",
      "Epoch 33/600\n",
      "  Train -> BCE: 0.4595, MSE: 13413.6860, Jaccard: 0.6279\n",
      "  Val   -> BCE: 0.4462, MSE: 11657.1392, Jaccard: 0.6291\n",
      "Epoch 34/600\n",
      "  Train -> BCE: 0.4575, MSE: 12861.2321, Jaccard: 0.6269\n",
      "  Val   -> BCE: 0.4663, MSE: 11400.7345, Jaccard: 0.6047\n",
      "Epoch 35/600\n",
      "  Train -> BCE: 0.4606, MSE: 12539.9469, Jaccard: 0.6249\n",
      "  Val   -> BCE: 0.4483, MSE: 11448.9996, Jaccard: 0.6338\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Jaccard Similarity Function\n",
    "def jaccard_similarity(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute the Jaccard Similarity (Intersection over Union) for multi-label classification.\n",
    "    \n",
    "    Args:\n",
    "        y_true (Tensor): Ground truth binary labels (0 or 1).\n",
    "        y_pred (Tensor): Predicted probabilities (before thresholding).\n",
    "        threshold (float): Threshold to convert probabilities to binary predictions.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average Jaccard similarity over the batch.\n",
    "    \"\"\"\n",
    "    y_pred_binary = (y_pred > threshold).float()\n",
    "    intersection = (y_true * y_pred_binary).sum(dim=1)\n",
    "    union = (y_true + y_pred_binary).clamp(0, 1).sum(dim=1)\n",
    "    jaccard_scores = intersection / (union + 1e-6)  # Add small epsilon to avoid division by zero\n",
    "    return jaccard_scores.mean().item()\n",
    "\n",
    "\n",
    "# Training Setup\n",
    "input_dim = X_train.shape[1]  # Number of input features\n",
    "output_dim = Y_train.shape[1] # Number of target genes\n",
    "\n",
    "hidden_dim = 1024\n",
    "num_hidden_layers = 4\n",
    "dropout_rate = 0.3\n",
    "l2_reg = 1e-4  # Weight decay (L2 regularization)\n",
    "\n",
    "# Instantiate Model\n",
    "model = DualBranchNet(input_dim, output_dim, hidden_dim, num_hidden_layers, dropout_rate).to(device)\n",
    "\n",
    "# Optimizer & Loss Functions\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_reg)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "bce_loss = nn.BCELoss()\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Convert Data to PyTorch DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 600\n",
    "alpha = 0.5  # Weight for BCE loss vs. MSE loss\n",
    "threshold = 0.5  # Threshold for Jaccard computation\n",
    "\n",
    "\n",
    "# Lists to store training progress\n",
    "bce_train_history = []\n",
    "mse_train_history = []\n",
    "jaccard_train_history = []\n",
    "\n",
    "bce_val_history = []\n",
    "mse_val_history = []\n",
    "jaccard_val_history = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_bce_loss_train = 0\n",
    "    total_mse_loss_train = 0\n",
    "    total_jaccard_train = 0\n",
    "\n",
    "    # Training Phase\n",
    "    for batch in train_loader:\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #### First pass - Classification update (BCE) ####\n",
    "        prob_y_positive, y_regression = model(x_batch)\n",
    "\n",
    "        target_binary = (y_batch > 0).float()  # Binary mask\n",
    "        loss_classification = bce_loss(prob_y_positive, target_binary)\n",
    "\n",
    "        # Compute Jaccard similarity\n",
    "        jaccard_score = jaccard_similarity(target_binary, prob_y_positive, threshold)\n",
    "\n",
    "        # Perform gradient update for BCE\n",
    "        loss_classification.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_bce_loss_train += loss_classification.item()\n",
    "        total_jaccard_train += jaccard_score\n",
    "\n",
    "        #### Second pass - Regression update (MSE) ####\n",
    "        optimizer.zero_grad()  # Clear gradients again\n",
    "\n",
    "        # Forward pass again\n",
    "        prob_y_positive, y_regression = model(x_batch)\n",
    "\n",
    "        # Compute MSE only for positive Y\n",
    "        mask = target_binary.bool()  # Convert to boolean mask\n",
    "        if mask.sum() > 0:  # If there are positive values\n",
    "            loss_regression = mse_loss(y_regression[mask], y_batch[mask])\n",
    "        else:\n",
    "            loss_regression = torch.tensor(0.0, device=device)\n",
    "\n",
    "        # Perform gradient update for MSE\n",
    "        loss_regression.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_mse_loss_train += loss_regression.item()\n",
    "\n",
    "    #### Validation Phase ####\n",
    "    model.eval()\n",
    "    total_bce_loss_val = 0\n",
    "    total_mse_loss_val = 0\n",
    "    total_jaccard_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            prob_y_positive, y_regression = model(x_batch)\n",
    "\n",
    "            target_binary = (y_batch > 0).float()\n",
    "\n",
    "            # Compute BCE Loss\n",
    "            loss_classification = bce_loss(prob_y_positive, target_binary)\n",
    "            total_bce_loss_val += loss_classification.item()\n",
    "\n",
    "            # Compute Jaccard Similarity\n",
    "            jaccard_score = jaccard_similarity(target_binary, prob_y_positive, threshold)\n",
    "            total_jaccard_val += jaccard_score\n",
    "\n",
    "            # Compute MSE Loss\n",
    "            mask = target_binary.bool()\n",
    "            if mask.sum() > 0:\n",
    "                loss_regression = mse_loss(y_regression[mask], y_batch[mask])\n",
    "            else:\n",
    "                loss_regression = torch.tensor(0.0, device=device)\n",
    "\n",
    "            total_mse_loss_val += loss_regression.item()\n",
    "\n",
    "    #### Logging & Storing Metrics ####\n",
    "    num_train_batches = len(train_loader)\n",
    "    num_val_batches = len(val_loader)\n",
    "\n",
    "    avg_bce_train = total_bce_loss_train / num_train_batches\n",
    "    avg_mse_train = total_mse_loss_train / num_train_batches\n",
    "    avg_jaccard_train = total_jaccard_train / num_train_batches\n",
    "\n",
    "    avg_bce_val = total_bce_loss_val / num_val_batches\n",
    "    avg_mse_val = total_mse_loss_val / num_val_batches\n",
    "    avg_jaccard_val = total_jaccard_val / num_val_batches\n",
    "\n",
    "    # Store the metrics for plotting\n",
    "    bce_train_history.append(avg_bce_train)\n",
    "    mse_train_history.append(avg_mse_train)\n",
    "    jaccard_train_history.append(avg_jaccard_train)\n",
    "\n",
    "    bce_val_history.append(avg_bce_val)\n",
    "    mse_val_history.append(avg_mse_val)\n",
    "    jaccard_val_history.append(avg_jaccard_val)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"  Train -> BCE: {avg_bce_train:.4f}, MSE: {avg_mse_train:.4f}, Jaccard: {avg_jaccard_train:.4f}\")\n",
    "    print(f\"  Val   -> BCE: {avg_bce_val:.4f}, MSE: {avg_mse_val:.4f}, Jaccard: {avg_jaccard_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c607cd9-6c8e-48bb-9448-1f44240322f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(bce_train_history, ls='-', label='BCE Train', color='C0')\n",
    "plt.plot(bce_val_history, ls='--', label='BCE Val', color='C0', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(jaccard_train_history, ls='-', label='Jaccard Train', color='C2')\n",
    "plt.plot(jaccard_val_history, ls='--',  label='Jaccard Val', color='C2', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(mse_train_history, ls='-', label='MSE Train', color='C1')\n",
    "plt.plot(mse_val_history, ls='--',  label='MSE Val', color='C1', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cb9dd-9a33-4171-b3eb-c3ccd13b448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('model.pth'), strict=False)\n",
    "model.eval()  # Set to evaluation mode if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada142d-ecd7-47c9-a004-048635e74591",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14bb528-dffc-4446-9a76-0cbddf56ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    prob_y_positive, _ = model(X_test.to(device)) \n",
    "\n",
    "all_preds_np = prob_y_positive.cpu().numpy()\n",
    "all_labels_np = (Y_test.cpu().numpy() > 0).astype(int)\n",
    "\n",
    "num_outputs = all_preds_np.shape[1]\n",
    "roc_aucs = np.full(num_outputs, np.nan)  # Initialize with NaNs\n",
    "\n",
    "for i in tqdm(range(num_outputs)):\n",
    "    if len(np.unique(all_labels_np[:, i])) > 1:  # Ensure at least two classes exist\n",
    "        roc_aucs[i] = roc_auc_score(all_labels_np[:, i], all_preds_np[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f72eaf-7958-4ad4-8859-9739ca5c6fea",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8d86d-6dc3-4ac9-995f-21397771cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(roc_aucs[~np.isnan(roc_aucs)], bins=50, alpha=0.75, edgecolor='white')\n",
    "plt.xlabel(\"ROC-AUC Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of ROC-AUC Scores Across Genes\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean ROC-AUC Score: {np.mean(roc_aucs[~np.isnan(roc_aucs)])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab771e4-dc23-4685-868f-50bf8618ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(roc_aucs).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1924b-9c11-41b0-8b79-52d35ad71f08",
   "metadata": {},
   "source": [
    "### Correlation between Gene Frequency in samples and ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d94e0b-79c6-4185-a53c-d441efdbc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_occurrences = np.count_nonzero(Y_test.cpu().numpy(), axis=0)\n",
    "\n",
    "valid_indices = ~np.isnan(roc_aucs)\n",
    "filtered_occurrences = gene_occurrences[valid_indices]\n",
    "filtered_roc_aucs = roc_aucs[valid_indices]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(filtered_occurrences/Y_test.shape[0], filtered_roc_aucs, alpha=0.1)\n",
    "plt.xlabel(\"Number of occurrences in Y_test\")\n",
    "plt.ylabel(\"ROC-AUC Score\")\n",
    "plt.title(\"Gene Occurrences vs. ROC-AUC Score\")\n",
    "# plt.xscale(\"log\")  # Log scale for better visualization if needed\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c95d0-0ef4-4a3f-bccc-c4fd5db7333d",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a5f95-73b2-4010-bf35-251f1da8b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model.predict(X_test.to(device))\n",
    "\n",
    "# Move everything to CPU for evaluation\n",
    "y_pred_np = y_pred.cpu().numpy()\n",
    "y_true_np = Y_test.cpu().numpy()\n",
    "\n",
    "num_outputs = y_pred_np.shape[1]\n",
    "\n",
    "# Compute Metrics\n",
    "mse_values = np.full(num_outputs, np.nan)\n",
    "mae_values = np.full(num_outputs, np.nan)\n",
    "r2_values = np.full(num_outputs, np.nan)\n",
    "spearman_corrs = np.full(num_outputs, np.nan)\n",
    "explained_var = np.full(num_outputs, np.nan)\n",
    "\n",
    "for i in tqdm(range(num_outputs), desc=\"Evaluating Regression Quality\"):\n",
    "    y_true_col = y_true_np[:, i]\n",
    "    y_pred_col = y_pred_np[:, i]\n",
    "\n",
    "    # Check if the target values vary\n",
    "    if np.var(y_true_col) > 0:\n",
    "        mse_values[i] = mean_squared_error(y_true_col, y_pred_col)\n",
    "        mae_values[i] = mean_absolute_error(y_true_col, y_pred_col)\n",
    "        r2_values[i] = r2_score(y_true_col, y_pred_col)\n",
    "        explained_var[i] = explained_variance_score(y_true_col, y_pred_col)\n",
    "\n",
    "        # Only compute Spearman correlation if there is variation\n",
    "        if len(np.unique(y_true_col)) > 1 and len(np.unique(y_pred_col)) > 1:\n",
    "            spearman_corrs[i] = spearmanr(y_true_col, y_pred_col).correlation\n",
    "\n",
    "print(\"\\n**Overall Regression Evaluation:**\")\n",
    "print(f\"Mean MSE: {np.nanmean(mse_values):.6f}\")\n",
    "print(f\"Mean MAE: {np.nanmean(mae_values):.6f}\")\n",
    "print(f\"Mean R² Score: {np.nanmean(r2_values):.6f}\")\n",
    "print(f\"Mean Spearman Correlation: {np.nanmean(spearman_corrs):.6f}\")\n",
    "print(f\"Mean Explained Variance: {np.nanmean(explained_var):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295d903-d86e-4fe8-a9a9-d8130ada6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# MSE Distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(mse_values[~np.isnan(mse_values)], bins=50, alpha=0.75)\n",
    "plt.xlabel(\"MSE\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.yscale('log')\n",
    "plt.title(\"Distribution of MSE\")\n",
    "\n",
    "# MAE Distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(mae_values[~np.isnan(mae_values)], bins=50, alpha=0.75)\n",
    "plt.xlabel(\"MAE\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.yscale('log')\n",
    "plt.title(\"Distribution of MAE\")\n",
    "\n",
    "# Spearman Correlation Distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(spearman_corrs[~np.isnan(spearman_corrs)], bins=50, alpha=0.75)\n",
    "plt.xlabel(\"Spearman Correlation\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Spearman Correlation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
